{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:41:57.117054Z",
     "start_time": "2020-03-20T14:41:56.386956Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "%matplotlib inline\n",
    "\n",
    "# Use proxy and headers for safe web scraping\n",
    "# os.environ['HTTPS_PROXY'] = 'http://3.112.188.39:8080'\n",
    "\n",
    "# pd.options.mode.chained_assignment = None\n",
    "headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/'\n",
    "    '537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:41:57.120385Z",
     "start_time": "2020-03-20T14:41:57.118300Z"
    }
   },
   "outputs": [],
   "source": [
    "countries_link = {'USA':'https://www.amazon.com',\n",
    "                  'Australia':'https://www.amazon.com.au',\n",
    "                  'UK':'https://www.amazon.co.uk',\n",
    "                  'India':'https://www.amazon.in',\n",
    "                  'Japan':'https://www.amazon.co.jp/',\n",
    "                  'UAE':'https://amazon.ae'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:00.192930Z",
     "start_time": "2020-03-20T14:42:00.177922Z"
    }
   },
   "outputs": [],
   "source": [
    "amazon_usa = {'health_and_beauty':{'hair_products':{'shampoo':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11057241%2Cn%3A17911764011%2Cn%3A11057651&dc&',\n",
    "                                                    'conditioner':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11057241%2Cn%3A17911764011%2Cn%3A11057251&dc&',\n",
    "                                                    'hair_scalp_treatment':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11057241%2Cn%3A11057431&dc&',\n",
    "                                                    'treatment_oil':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11057241%2Cn%3A10666439011&dc&',\n",
    "                                                    'hair_loss':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11057241%2Cn%3A10898755011&dc&'},\n",
    "                                   'skin_care':{'body':{'cleansers':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060521%2Cn%3A11056281&dc&',\n",
    "                                                        'moisturizers':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060521%2Cn%3A11060661&dc&',\n",
    "                                                        'treatments':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060521%2Cn%3A11056421&dc&'},\n",
    "                                                'eyes':{'creams':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11061941%2Cn%3A7730090011&dc&',\n",
    "                                                        'gels':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11061941%2Cn%3A7730092011&dc&',\n",
    "                                                        'serums':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11061941%2Cn%3A7730098011&dc&'},\n",
    "                                                'face':{'f_cleansers':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060711%2Cn%3A11060901&dc&',\n",
    "                                                        'f_moisturizers':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060711%2Cn%3A11060901&dc&',\n",
    "                                                        'scrubs':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060711%2Cn%3A11061091&dc&',\n",
    "                                                        'toners':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060711%2Cn%3A11061931&dc&',\n",
    "                                                        'f_treatments':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A11060711%2Cn%3A11061931&dc&'},\n",
    "                                                'lipcare':'https://www.amazon.com/s?i=beauty-intl-ship&bbn=16225006011&rh=n%3A%2116225006011%2Cn%3A11060451%2Cn%3A3761351&dc&'}},\n",
    "              'food':{'tea':{'herbal':'https://www.amazon.com/s?k=tea&i=grocery&rh=n%3A16310101%2Cn%3A16310231%2Cn%3A16521305011%2Cn%3A16318401%2Cn%3A16318511&dc&',\n",
    "                             'green':'https://www.amazon.com/s?k=tea&i=grocery&rh=n%3A16310101%2Cn%3A16310231%2Cn%3A16521305011%2Cn%3A16318401%2Cn%3A16318471&dc&',\n",
    "                             'black':'https://www.amazon.com/s?k=tea&i=grocery&rh=n%3A16310101%2Cn%3A16310231%2Cn%3A16521305011%2Cn%3A16318401%2Cn%3A16318411&dc&',\n",
    "                             'chai':'https://www.amazon.com/s?k=tea&i=grocery&rh=n%3A16310101%2Cn%3A16310231%2Cn%3A16521305011%2Cn%3A16318401%2Cn%3A348022011&dc&'},\n",
    "                      'coffee':'https://www.amazon.com/s?k=tea&i=grocery&rh=n%3A16310101%2Cn%3A16310231%2Cn%3A16521305011%2Cn%3A16318031%2Cn%3A2251593011&dc&',\n",
    "                      'dried_fruits':{'mixed':'https://www.amazon.com/s?k=dried+fruits&i=grocery&rh=n%3A16310101%2Cn%3A6506977011%2Cn%3A9865332011%2Cn%3A9865334011%2Cn%3A9865348011&dc&',\n",
    "                                      'mangoes':'https://www.amazon.com/s?k=dried+fruits&rh=n%3A16310101%2Cn%3A9865346011&dc&'},\n",
    "                      'nuts':{'mixed':'https://www.amazon.com/s?k=nuts&rh=n%3A16310101%2Cn%3A16322931&dc&',\n",
    "                              'peanuts':'https://www.amazon.com/s?k=nuts&i=grocery&rh=n%3A16310101%2Cn%3A18787303011%2Cn%3A16310221%2Cn%3A16322881%2Cn%3A16322941&dc&',\n",
    "                              'cashews':'https://www.amazon.com/s?k=nuts&i=grocery&rh=n%3A16310101%2Cn%3A18787303011%2Cn%3A16310221%2Cn%3A16322881%2Cn%3A16322901&dc&'}},\n",
    "              'supplements':{'sports':{'pre_workout':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A6973663011%2Cn%3A6973697011&dc&',\n",
    "                                       'protein':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A6973663011%2Cn%3A6973704011&dc&',\n",
    "                                       'fat_burner':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A6973663011%2Cn%3A6973679011&dc&',\n",
    "                                       'weight_gainer':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A6973663011%2Cn%3A6973725011&dc&'},\n",
    "                             'vitamins_dietary':{'supplements':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A3764441%2Cn%3A6939426011&dc&',\n",
    "                                                 'multivitamins':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A3774861&dc&'}},\n",
    "              'wellness':{'ayurveda':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A10079996011%2Cn%3A13052911%2Cn%3A13052941&dc&',\n",
    "                          'essential_oil_set':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A10079996011%2Cn%3A13052911%2Cn%3A18502613011&dc&',\n",
    "                          'massage_oil':'https://www.amazon.com/s?k=supplements&i=hpc&rh=n%3A3760901%2Cn%3A10079996011%2Cn%3A14442631&dc&'},\n",
    "              'personal_accessories':{'bags':{'women':{'clutches':'https://www.amazon.com/s?k=bags&i=fashion-womens-handbags&bbn=15743631&rh=n%3A7141123011%2Cn%3A%217141124011%2Cn%3A7147440011%2Cn%3A15743631%2Cn%3A17037745011&dc&',\n",
    "                                                       'crossbody':'https://www.amazon.com/s?k=bags&i=fashion-womens-handbags&bbn=15743631&rh=n%3A7141123011%2Cn%3A%217141124011%2Cn%3A7147440011%2Cn%3A15743631%2Cn%3A2475899011&dc&',\n",
    "                                                       'fashion':'https://www.amazon.com/s?k=bags&i=fashion-womens-handbags&bbn=15743631&rh=n%3A7141123011%2Cn%3A%217141124011%2Cn%3A7147440011%2Cn%3A15743631%2Cn%3A16977745011&dc&',\n",
    "                                                       'hobo':'https://www.amazon.com/s?k=bags&i=fashion-womens-handbags&bbn=15743631&rh=n%3A7141123011%2Cn%3A%217141124011%2Cn%3A7147440011%2Cn%3A15743631%2Cn%3A16977747011&dc&'}},\n",
    "                                      'jewelry':{'anklets':'https://www.amazon.com/s?i=fashion-womens-intl-ship&bbn=16225018011&rh=n%3A16225018011%2Cn%3A7192394011%2Cn%3A7454897011&dc&',\n",
    "                                                 'bracelets':'https://www.amazon.com/s?i=fashion-womens-intl-ship&bbn=16225018011&rh=n%3A16225018011%2Cn%3A7192394011%2Cn%3A7454898011&dc&',\n",
    "                                                 'earrings':'https://www.amazon.com/s?i=fashion-womens-intl-ship&bbn=16225018011&rh=n%3A16225018011%2Cn%3A7192394011%2Cn%3A7454917011&dc&',\n",
    "                                                 'necklaces':'https://www.amazon.com/s?i=fashion-womens-intl-ship&bbn=16225018011&rh=n%3A16225018011%2Cn%3A7192394011%2Cn%3A7454917011&dc&',\n",
    "                                                 'rings':'https://www.amazon.com/s?i=fashion-womens-intl-ship&bbn=16225018011&rh=n%3A16225018011%2Cn%3A7192394011%2Cn%3A7454939011&dc&'},\n",
    "                                      'artisan_fabrics':'https://www.amazon.com/s?k=fabrics&rh=n%3A2617941011%2Cn%3A12899121&dc&'}}\n",
    "amazon_uk = {'health_and_beauty':{'hair_products':{'shampoo':'https://www.amazon.co.uk/b/ref=amb_link_5?ie=UTF8&node=74094031&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_s=merchandised-search-leftnav&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_t=101&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_i=66469031',\n",
    "                                                   'conditioner':'https://www.amazon.co.uk/b/ref=amb_link_6?ie=UTF8&node=2867976031&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_s=merchandised-search-leftnav&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_t=101&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_i=66469031',\n",
    "                                                   'hair_loss':'https://www.amazon.co.uk/b/ref=amb_link_11?ie=UTF8&node=2867979031&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_s=merchandised-search-leftnav&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_t=101&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_i=66469031',\n",
    "                                                   'hair_scalp_treatment':'https://www.amazon.co.uk/b/ref=amb_link_7?ie=UTF8&node=2867977031&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_s=merchandised-search-leftnav&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_t=101&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_i=66469031',\n",
    "                                                   'treatment_oil':'https://www.amazon.co.uk/hair-oil-argan/b/ref=amb_link_8?ie=UTF8&node=2867981031&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_s=merchandised-search-leftnav&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_r=KF9SM53J2HXHP4EJD3AH&pf_rd_t=101&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_p=aaaa7182-fdd6-4b35-8f0b-993e78880b69&pf_rd_i=66469031'},\n",
    "                                  'skin_care':{'body':{'cleanser':'https://www.amazon.co.uk/s/ref=lp_344269031_nr_n_3?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A344269031%2Cn%3A344282031&bbn=344269031&ie=UTF8&qid=1581612722&rnid=344269031',\n",
    "                                                       'moisturizers':'https://www.amazon.co.uk/s/ref=lp_344269031_nr_n_1?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A344269031%2Cn%3A2805272031&bbn=344269031&ie=UTF8&qid=1581612722&rnid=344269031'},\n",
    "                                               'eyes':{'creams':'https://www.amazon.co.uk/s/ref=lp_118465031_nr_n_0?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118465031%2Cn%3A344259031&bbn=118465031&ie=UTF8&qid=1581612984&rnid=118465031',\n",
    "                                                       'gels':'https://www.amazon.co.uk/s/ref=lp_118465031_nr_n_1?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118465031%2Cn%3A344258031&bbn=118465031&ie=UTF8&qid=1581613044&rnid=118465031',\n",
    "                                                       'serums':'https://www.amazon.co.uk/s/ref=lp_118465031_nr_n_3?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118465031%2Cn%3A344257031&bbn=118465031&ie=UTF8&qid=1581613044&rnid=118465031'},\n",
    "                                               'face':{'cleansers':'https://www.amazon.co.uk/s/ref=lp_118466031_nr_n_1?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118466031%2Cn%3A344265031&bbn=118466031&ie=UTF8&qid=1581613120&rnid=118466031',\n",
    "                                                       'moisturizers':'https://www.amazon.co.uk/s/ref=lp_118466031_nr_n_3?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118466031%2Cn%3A2805291031&bbn=118466031&ie=UTF8&qid=1581613120&rnid=118466031',\n",
    "                                                       'toners':'https://www.amazon.co.uk/s/ref=lp_118466031_nr_n_0?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118466031%2Cn%3A344267031&bbn=118466031&ie=UTF8&qid=1581613120&rnid=118466031',\n",
    "                                                       'f_treatments':'https://www.amazon.co.uk/s?bbn=118466031&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118466031%2Cn%3A18918424031&dc&fst=as%3Aoff&qid=1581613120&rnid=118466031&ref=lp_118466031_nr_n_7'},\n",
    "                                               'lipcare':'https://www.amazon.co.uk/s/ref=lp_118464031_nr_n_4?fst=as%3Aoff&rh=n%3A117332031%2Cn%3A%21117333031%2Cn%3A118464031%2Cn%3A118467031&bbn=118464031&ie=UTF8&qid=1581613357&rnid=118464031'}},\n",
    "             'food':{'tea':{'herbal':'https://www.amazon.co.uk/s?k=tea&i=grocery&rh=n%3A340834031%2Cn%3A358584031%2Cn%3A11711401%2Cn%3A406567031&dc&qid=1581613483&rnid=344155031&ref=sr_nr_n_1',\n",
    "                            'green':'https://www.amazon.co.uk/s?k=tea&i=grocery&rh=n%3A340834031%2Cn%3A358584031%2Cn%3A11711401%2Cn%3A406566031&dc&qid=1581613483&rnid=344155031&ref=sr_nr_n_3',\n",
    "                            'black':'https://www.amazon.co.uk/s?k=tea&i=grocery&rh=n%3A340834031%2Cn%3A358584031%2Cn%3A11711401%2Cn%3A406564031&dc&qid=1581613483&rnid=344155031&ref=sr_nr_n_2'},\n",
    "                     'coffee':'https://www.amazon.co.uk/s?k=coffee&rh=n%3A340834031%2Cn%3A11711391&dc&qid=1581613715&rnid=1642204031&ref=sr_nr_n_2',\n",
    "                     'dried_fruits':{'mixed':'https://www.amazon.co.uk/s?k=dried+fruits&rh=n%3A340834031%2Cn%3A9733163031&dc&qid=1581613770&rnid=1642204031&ref=sr_nr_n_2'},\n",
    "                     'nuts':{'mixed':'https://www.amazon.co.uk/s?k=mixed&rh=n%3A359964031&ref=nb_sb_noss',\n",
    "                             'peanuts':'https://www.amazon.co.uk/s?k=peanuts&rh=n%3A359964031&ref=nb_sb_noss',\n",
    "                             'cashews':'https://www.amazon.co.uk/s?k=cashew&rh=n%3A359964031&ref=nb_sb_noss'}},\n",
    "             'supplements':{'sports':{'pre_workout':'https://www.amazon.co.uk/b/?node=5977685031&ref_=Oct_s9_apbd_odnav_hd_bw_b35Hc3L_1&pf_rd_r=C5MZHH5TH5F868B6FQWD&pf_rd_p=8086b6c9-ae16-5c3c-a879-030afa4ee08f&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=2826478031',\n",
    "                                      'protein':'https://www.amazon.co.uk/b/?node=2826510031&ref_=Oct_s9_apbd_odnav_hd_bw_b35Hc3L_0&pf_rd_r=C5MZHH5TH5F868B6FQWD&pf_rd_p=8086b6c9-ae16-5c3c-a879-030afa4ee08f&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=2826478031',\n",
    "                                      'fat_burner':'https://www.amazon.co.uk/b/?node=5977737031&ref_=Oct_s9_apbd_odnav_hd_bw_b35Hc3L_2&pf_rd_r=C5MZHH5TH5F868B6FQWD&pf_rd_p=8086b6c9-ae16-5c3c-a879-030afa4ee08f&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=2826478031'},\n",
    "                            'vitamins_dietary':{'supplements':'https://www.amazon.co.uk/b/?_encoding=UTF8&node=2826534031&bbn=65801031&ref_=Oct_s9_apbd_odnav_hd_bw_b35Hdc7_2&pf_rd_r=AY01DQVCB4SE7VVE7MTK&pf_rd_p=1ecdbf02-af23-502a-b7ab-9916ddd6690c&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=2826484031',\n",
    "                                                'multivitamins':'https://www.amazon.co.uk/b/?_encoding=UTF8&node=2826506031&bbn=65801031&ref_=Oct_s9_apbd_odnav_hd_bw_b35Hdc7_1&pf_rd_r=AY01DQVCB4SE7VVE7MTK&pf_rd_p=1ecdbf02-af23-502a-b7ab-9916ddd6690c&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=2826484031'}},\n",
    "             'wellness':{'massage_oil':'https://www.amazon.co.uk/b/?node=3360479031&ref_=Oct_s9_apbd_odnav_hd_bw_b50nmJ_4&pf_rd_r=GYVYF52HT2004EDTY67W&pf_rd_p=3f8e4361-c00b-588b-a07d-ff259bf98bbc&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=74073031',\n",
    "                         'ayurveda':'https://www.amazon.co.uk/s?k=ayurveda&rh=n%3A65801031%2Cn%3A2826449031&dc&qid=1581686978&rnid=1642204031&ref=sr_nr_n_22'},\n",
    "             'personal_accessories':{'bags':{'women':{'clutches':'https://www.amazon.co.uk/b/?node=1769563031&ref_=Oct_s9_apbd_odnav_hd_bw_b1vkt8h_3&pf_rd_r=VC8RX89R4V4JJ5TEBANF&pf_rd_p=cefca17f-8dac-5c80-848f-812aff1bfdd7&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=1769559031',\n",
    "                                                      'crossbody':'https://www.amazon.co.uk/b/?node=1769564031&ref_=Oct_s9_apbd_odnav_hd_bw_b1vkt8h_1&pf_rd_r=VC8RX89R4V4JJ5TEBANF&pf_rd_p=cefca17f-8dac-5c80-848f-812aff1bfdd7&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=1769559031',\n",
    "                                                      'fashion':'https://www.amazon.co.uk/b/?node=1769560031&ref_=Oct_s9_apbd_odnav_hd_bw_b1vkt8h_5&pf_rd_r=VC8RX89R4V4JJ5TEBANF&pf_rd_p=cefca17f-8dac-5c80-848f-812aff1bfdd7&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=1769559031',\n",
    "                                                      'hobo':'https://www.amazon.co.uk/b/?node=1769565031&ref_=Oct_s9_apbd_odnav_hd_bw_b1vkt8h_4&pf_rd_r=VC8RX89R4V4JJ5TEBANF&pf_rd_p=cefca17f-8dac-5c80-848f-812aff1bfdd7&pf_rd_s=merchandised-search-11&pf_rd_t=BROWSE&pf_rd_i=1769559031'}},\n",
    "                                     'jewelry':{'anklets':'https://www.amazon.co.uk/s/ref=lp_10382835031_nr_n_0?fst=as%3Aoff&rh=n%3A193716031%2Cn%3A%21193717031%2Cn%3A10382835031%2Cn%3A10382860031&bbn=10382835031&ie=UTF8&qid=1581687575&rnid=10382835031',\n",
    "                                                'bracelets':'https://www.amazon.co.uk/s/ref=lp_10382835031_nr_n_1?fst=as%3Aoff&rh=n%3A193716031%2Cn%3A%21193717031%2Cn%3A10382835031%2Cn%3A10382861031&bbn=10382835031&ie=UTF8&qid=1581687575&rnid=10382835031',\n",
    "                                                'earrings':'https://www.amazon.co.uk/s/ref=lp_10382835031_nr_n_4?fst=as%3Aoff&rh=n%3A193716031%2Cn%3A%21193717031%2Cn%3A10382835031%2Cn%3A10382865031&bbn=10382835031&ie=UTF8&qid=1581687575&rnid=10382835031',\n",
    "                                                'necklaces':'https://www.amazon.co.uk/s/ref=lp_10382835031_nr_n_7?fst=as%3Aoff&rh=n%3A193716031%2Cn%3A%21193717031%2Cn%3A10382835031%2Cn%3A10382868031&bbn=10382835031&ie=UTF8&qid=1581687575&rnid=10382835031',\n",
    "                                                'rings':'https://www.amazon.co.uk/s/ref=lp_10382835031_nr_n_10?fst=as%3Aoff&rh=n%3A193716031%2Cn%3A%21193717031%2Cn%3A10382835031%2Cn%3A10382871031&bbn=10382835031&ie=UTF8&qid=1581687575&rnid=10382835031'},\n",
    "                                     'artisan_fabrics':'https://www.amazon.co.uk/s?k=fabric&rh=n%3A11052681%2Cn%3A3063518031&dc&qid=1581687726&rnid=1642204031&ref=a9_sc_1'}}\n",
    "amazon_india = {'health_and_beauty':{'hair_products':{'shampoo':'https://www.amazon.in/b/ref=s9_acss_bw_cg_btyH1_2a1_w?ie=UTF8&node=1374334031&pf_rd_m=A1K21FY43GMZF8&pf_rd_s=merchandised-search-5&pf_rd_r=JHDJ4QHM0APVS05NGF4G&pf_rd_t=101&pf_rd_p=41b9c06b-1514-47de-a1c6-f4f13fb55ffe&pf_rd_i=1374305031',\n",
    "                                                      'conditioner':'https://www.amazon.in/b/ref=s9_acss_bw_cg_btyH1_2b1_w?ie=UTF8&node=1374306031&pf_rd_m=A1K21FY43GMZF8&pf_rd_s=merchandised-search-5&pf_rd_r=CBABMCW6C69JRBGZNWWP&pf_rd_t=101&pf_rd_p=41b9c06b-1514-47de-a1c6-f4f13fb55ffe&pf_rd_i=1374305031',\n",
    "                                                      'treatment_oil':''},\n",
    "                                     'skin_care':[],\n",
    "                                     'wellness_product':[]},\n",
    "                'food':{'tea':[],\n",
    "                        'coffee':[],\n",
    "                        'dried_fruits':[],\n",
    "                        'nuts':[],\n",
    "                        'supplements':[]},\n",
    "                'personal_accessories':{'bags':[],\n",
    "                                        'jewelry':[],\n",
    "                                        'artisan_fabrics':[]}}\n",
    "amazon_aus = {'health_and_beauty':{'hair_products':{'shampoo':'https://www.amazon.com.au/b/?_encoding=UTF8&node=5150253051&bbn=4851917051&ref_=Oct_s9_apbd_odnav_hd_bw_b5cXATz&pf_rd_r=6SEM7GFDN7CQ2W4KXM9M&pf_rd_p=9dd4b462-1094-5e36-890d-bb1b694c8b53&pf_rd_s=merchandised-search-12&pf_rd_t=BROWSE&pf_rd_i=5150070051',\n",
    "                                                    'conditioner':'https://www.amazon.com.au/b/?_encoding=UTF8&node=5150226051&bbn=4851917051&ref_=Oct_s9_apbd_odnav_hd_bw_b5cXATz&pf_rd_r=6SEM7GFDN7CQ2W4KXM9M&pf_rd_p=9dd4b462-1094-5e36-890d-bb1b694c8b53&pf_rd_s=merchandised-search-12&pf_rd_t=BROWSE&pf_rd_i=5150070051'},\n",
    "                                    'skin_care':{'body':{'cleansers':'',\n",
    "                                                        'moisturizers':'',\n",
    "                                                        'treatments':''},\n",
    "                                                'eyes':{'creams':'',\n",
    "                                                        'gels':'',\n",
    "                                                        'serums':''},\n",
    "                                                'face':{'f_cleansers':'https://www.amazon.com.au/s/ref=lp_5150144051_nr_n_0?fst=as%3Aoff&rh=n%3A4851567051%2Cn%3A%214851568051%2Cn%3A5150073051%2Cn%3A5150144051%2Cn%3A5150424051&bbn=5150144051&ie=UTF8&qid=1584163882&rnid=5150144051',\n",
    "                                                        'f_moisturizers':'https://www.amazon.com.au/s/ref=lp_5150144051_nr_n_2?fst=as%3Aoff&rh=n%3A4851567051%2Cn%3A%214851568051%2Cn%3A5150073051%2Cn%3A5150144051%2Cn%3A5150428051&bbn=5150144051&ie=UTF8&qid=1584121067&rnid=5150144051',\n",
    "                                                        'scrubs':'',\n",
    "                                                        'toners':'',\n",
    "                                                        'f_treatments':'https://www.amazon.com.au/s/ref=lp_5150144051_nr_n_3?fst=as%3Aoff&rh=n%3A4851567051%2Cn%3A%214851568051%2Cn%3A5150073051%2Cn%3A5150144051%2Cn%3A5150430051&bbn=5150144051&ie=UTF8&qid=1584180807&rnid=5150144051'},\n",
    "                                                'lipcare':''}},              \n",
    "              'food':{'tea':[],\n",
    "                        'coffee':[],\n",
    "                        'dried_fruits':[],\n",
    "                        'nuts':[],\n",
    "                        'supplements':[]},\n",
    "                'personal_accessories':{'bags':[],\n",
    "                                        'jewelry':[],\n",
    "                                        'artisan_fabrics':[]}}\n",
    "\n",
    "amazon = {'USA':amazon_usa,\n",
    "          'UK':amazon_uk,\n",
    "          'India':amazon_india,\n",
    "          'Australia':amazon_aus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:00.350202Z",
     "start_time": "2020-03-20T14:42:00.337476Z"
    }
   },
   "outputs": [],
   "source": [
    "def hover(browser, xpath):\n",
    "    '''\n",
    "    This function makes an automated mouse hovering in the selenium webdriver\n",
    "    element based on its xpath.\n",
    "    \n",
    "    PARAMETER\n",
    "    ---------\n",
    "    browser: Selenium based webbrowser\n",
    "    xpath: str\n",
    "        xpath of the element in the webpage where hover operation has to be \n",
    "        performed.\n",
    "    '''\n",
    "    element_to_hover_over = browser.find_element_by_xpath(xpath)\n",
    "    hover = ActionChains(browser).move_to_element(element_to_hover_over)\n",
    "    hover.perform()\n",
    "    element_to_hover_over.click()\n",
    "    \n",
    "def browser(link):\n",
    "    '''This funtion opens a selenium based chromebrowser specifically tuned \n",
    "    to work for amazon product(singular item) webpages. Few functionality \n",
    "    includes translation of webpage, clicking the initial popups, and hovering\n",
    "    over product imagesso that the images can be scrape\n",
    "    \n",
    "    PARAMETER\n",
    "    ---------\n",
    "    link: str\n",
    "        Amazon Product item link\n",
    "        \n",
    "    RETURN\n",
    "    ------\n",
    "    driver: Selenium web browser with operated functions\n",
    "    '''\n",
    "    options = Options()\n",
    "    prefs = {\n",
    "      \"translate_whitelists\": {\"ja\":\"en\",\"de\":'en'},\n",
    "      \"translate\":{\"enabled\":\"true\"}\n",
    "    }\n",
    "#     helium = r'C:\\Users\\Dell-pc\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions\\njmehopjdpcckochcggncklnlmikcbnb\\4.2.12_0'\n",
    "#     options.add_argument(helium)\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.get(link)\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"nav-main\"]/div[1]/div[2]/div/div[3]/span[1]/span/input').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[3]')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[4]')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[5]')\n",
    "    except:\n",
    "        pass  \n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[6]')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:    \n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[7]')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[8]')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        hover(driver,'//*[@id=\"altImages\"]/ul/li[9]')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"a-popover-6\"]/div/header/button/i').click()\n",
    "    except:\n",
    "        pass\n",
    "    return driver\n",
    "\n",
    "def scroll_temp(driver):\n",
    "    ''' \n",
    "    Automated Scroller in Selenium Webbrowser\n",
    "    \n",
    "    PARAMETER\n",
    "    ---------\n",
    "    driver: Selenium Webbrowser\n",
    "    '''\n",
    "    pre_scroll_height = driver.execute_script('return document.body.scrollHeight;')\n",
    "    run_time, max_run_time = 0, 2\n",
    "    while True:\n",
    "        iteration_start = time.time()\n",
    "        # Scroll webpage, the 100 allows for a more 'aggressive' scroll\n",
    "        driver.execute_script('window.scrollTo(0,0.6*document.body.scrollHeight);')\n",
    "\n",
    "        post_scroll_height = driver.execute_script('return document.body.scrollHeight;')\n",
    "\n",
    "        scrolled = post_scroll_height != pre_scroll_height\n",
    "        timed_out = run_time >= max_run_time\n",
    "\n",
    "        if scrolled:\n",
    "            run_time = 0\n",
    "            pre_scroll_height = post_scroll_height\n",
    "        elif not scrolled and not timed_out:\n",
    "            run_time += time.time() - iteration_start\n",
    "        elif not scrolled and timed_out:\n",
    "            break\n",
    "\n",
    "# def scroll(driver):\n",
    "#     scroll_temp(driver)\n",
    "#     from selenium.common.exceptions import NoSuchElementException\n",
    "#     try:\n",
    "#         element = driver.find_element_by_xpath('//*[@id=\"reviewsMedley\"]/div/div[1]')\n",
    "#     except NoSuchElementException:\n",
    "#         try:\n",
    "#             element = driver.find_element_by_xpath('//*[@id=\"reviewsMedley\"]') \n",
    "#         except NoSuchElementException:\n",
    "#             element = driver.find_element_by_xpath('//*[@id=\"detail-bullets_feature_div\"]')\n",
    "#     actions = ActionChains(driver)\n",
    "#     actions.move_to_element(element).perform()\n",
    "    \n",
    "def scroll(driver):\n",
    "    scroll_temp(driver)\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "    try:\n",
    "        try:\n",
    "            element = driver.find_element_by_xpath('//*[@id=\"reviewsMedley\"]/div/div[1]')\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                element = driver.find_element_by_xpath('//*[@id=\"reviewsMedley\"]') \n",
    "            except NoSuchElementException:\n",
    "                element = driver.find_element_by_xpath('//*[@id=\"detail-bullets_feature_div\"]')\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(element).perform()\n",
    "    except NoSuchElementException:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:01.403040Z",
     "start_time": "2020-03-20T14:42:01.386113Z"
    }
   },
   "outputs": [],
   "source": [
    "def browser_link(product_link,country):\n",
    "    '''Returns all the web link of the products based on the first \n",
    "    page of the product category. It captures product link of all the pages for \n",
    "    that specific product.\n",
    "    \n",
    "    PARAMETER\n",
    "    ---------\n",
    "    link: str\n",
    "        The initial web link of the product page. This is generally the \n",
    "        first page of the all the items for that specfic product\n",
    "        \n",
    "    RETURN\n",
    "    ------\n",
    "    links: list\n",
    "        It is a list of strings which contains all the links of the items \n",
    "        for the specific product\n",
    "    \n",
    "    '''\n",
    "    driver = browser(product_link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "#     try:\n",
    "#         pages_soup = soup.findAll(\"ul\",{\"class\":\"a-pagination\"})\n",
    "#         pages = int(pages_soup[0].findAll(\"li\",{'class':'a-disabled'})[1].text)\n",
    "#     except:\n",
    "#         pass\n",
    "#     try:\n",
    "#         pages_soup = soup.findAll(\"div\",{\"id\":\"pagn\"})\n",
    "#         pages = int(pages_soup[0].findAll(\"span\",{'class':'pagnDisabled'})[0].text)\n",
    "#     except:\n",
    "#         try:\n",
    "#             pages_soup = soup.findAll(\"div\",{\"id\":\"pagn\"})\n",
    "#             pages = int(pages_soup[0].findAll(\"span\",{'class':'pagnDisabled'})[1].text)\n",
    "#         except:\n",
    "#             pass\n",
    "    pages = 400\n",
    "    print(pages)\n",
    "    links = []\n",
    "    for page in range(1,pages+1):\n",
    "        print(page)\n",
    "        link_page = product_link + '&page=' + str(page)\n",
    "        driver_temp = browser(link_page)\n",
    "        time.sleep(2)\n",
    "        soup_temp = BeautifulSoup(driver_temp.page_source, 'lxml')\n",
    "        try:\n",
    "            search = soup_temp.findAll(\"div\",{\"id\":\"mainResults\"})\n",
    "            temp_search = search[1].findAll(\"a\",{'class':'a-link-normal s-access-detail-page s-color-twister-title-link a-text-normal'})\n",
    "            for i in range(len(temp_search)):\n",
    "                if country == 'Australia':\n",
    "                    link = temp_search[i].get('href')\n",
    "                else:\n",
    "                    link = countries_link[country] + temp_search[i].get('href')\n",
    "                links.append(link)\n",
    "            print(len(links))\n",
    "        except:\n",
    "            try:\n",
    "                search = soup_temp.findAll(\"div\",{\"class\":\"s-result-list s-search-results sg-row\"})\n",
    "                temp_search = search[1].findAll(\"h2\")\n",
    "                if len(temp_search) < 2:\n",
    "                    for i in range(len(search[0].findAll(\"h2\"))):\n",
    "                        temp = search[0].findAll(\"h2\")[i]\n",
    "                        for j in range(len(temp.findAll('a'))):\n",
    "                            link = countries_link[country]+temp.findAll('a')[j].get('href')\n",
    "                            links.append(link)   \n",
    "                    print(len(links))\n",
    "                else:\n",
    "                    for i in range(len(search[1].findAll(\"h2\"))):\n",
    "                        temp = search[1].findAll(\"h2\")[i]\n",
    "                        for j in range(len(temp.findAll('a'))):\n",
    "                            link = countries_link[country]+temp.findAll('a')[j].get('href')\n",
    "                            links.append(link)   \n",
    "                    print(len(links))\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            search = soup_temp.findAll(\"div\",{\"id\":\"mainResults\"})\n",
    "            temp_search = search[0].findAll(\"a\",{'class':'a-link-normal s-access-detail-page s-color-twister-title-link a-text-normal'})\n",
    "            for i in range(len(temp_search)):\n",
    "                if country == 'Australia':\n",
    "                    link = temp_search[i].get('href')\n",
    "                else:\n",
    "                    link = countries_link[country] + temp_search[i].get('href')\n",
    "                links.append(link)\n",
    "            print(len(links))\n",
    "        except:\n",
    "            try:\n",
    "                search = soup_temp.findAll(\"div\",{\"class\":\"s-result-list s-search-results sg-row\"})\n",
    "                temp_search = search[1].findAll(\"h2\")\n",
    "                if len(temp_search) < 2:\n",
    "                    for i in range(len(search[0].findAll(\"h2\"))):\n",
    "                        temp = search[0].findAll(\"h2\")[i]\n",
    "                        for j in range(len(temp.findAll('a'))):\n",
    "                            link = countries_link[country]+temp.findAll('a')[j].get('href')\n",
    "                            links.append(link)   \n",
    "                    print(len(links))\n",
    "                else:\n",
    "                    for i in range(len(search[1].findAll(\"h2\"))):\n",
    "                        temp = search[1].findAll(\"h2\")[i]\n",
    "                        for j in range(len(temp.findAll('a'))):\n",
    "                            link = countries_link[country]+temp.findAll('a')[j].get('href')\n",
    "                            links.append(link)   \n",
    "                    print(len(links))\n",
    "\n",
    "            except:\n",
    "                print('Not Scrapable')\n",
    "                links = []\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:01.412132Z",
     "start_time": "2020-03-20T14:42:01.406270Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexes(amazon_links,link_list):\n",
    "    amazon_dict = amazon_links\n",
    "    if len(link_list) == 5:\n",
    "        return amazon_dict[link_list[0]][link_list[1]][link_list[2]][link_list[3]][link_list[4]]\n",
    "    elif len(link_list) == 4:\n",
    "        return amazon_dict[link_list[0]][link_list[1]][link_list[2]][link_list[3]]\n",
    "    elif len(link_list) == 3:\n",
    "        return amazon_dict[link_list[0]][link_list[1]][link_list[2]]\n",
    "    elif len(link_list) == 2:\n",
    "        return amazon_dict[link_list[0]][link_list[1]]\n",
    "    elif len(link_list) == 1:\n",
    "        return amazon_dict[link_list[0]]\n",
    "    else:\n",
    "        return print(\"Invalid Product\")\n",
    "    \n",
    "def products_links(country, **kwargs): \n",
    "    amazon_links = amazon[country]\n",
    "    directory_temp = []\n",
    "    for key, value in kwargs.items():\n",
    "        directory_temp.append(value)\n",
    "    directory = '/'.join(directory_temp)\n",
    "    print(directory)\n",
    "    product_link = indexes(amazon_links,directory_temp)\n",
    "    main_links = browser_link(product_link,country=country)\n",
    "    return main_links,directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:03.178925Z",
     "start_time": "2020-03-20T14:42:03.035459Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_images(filename):\n",
    "    import os\n",
    "    file_path = '/home/jishu/Amazon_UK/'\n",
    "    os.remove(file_path + filename)\n",
    "        \n",
    "def upload_s3(filename,key):\n",
    "    key_id = 'AKIAWR6YW7N5ZKW35OJI'\n",
    "    access_key = 'h/xrcI9A2SRU0ds+zts4EClKAqbzU+/iXdiDcgzm'\n",
    "    bucket_name = 'amazon-data-ecfullfill'\n",
    "    s3 = boto3.client('s3',aws_access_key_id=key_id,\n",
    "                      aws_secret_access_key=access_key)\n",
    "    try:\n",
    "        s3.upload_file(filename,bucket_name,key)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "def product_info(link,directory,country):\n",
    "    '''Get all the product information of an Amazon Product'''\n",
    "    \n",
    "    #Opening Selenium Webdrive with Amazon product\n",
    "    driver = browser(link)\n",
    "    time.sleep(4)\n",
    "    scroll(driver)\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Initializing BeautifulSoup operation in selenium browser\n",
    "    selenium_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Product Title\n",
    "    try:\n",
    "        product_title = driver.find_element_by_xpath('//*[@id=\"productTitle\"]').text\n",
    "    except:\n",
    "        product_title = 'Not Scrapable'\n",
    "    print(product_title)    \n",
    "\n",
    "    #Ratings - Star\n",
    "    try:\n",
    "        rating_star = float(selenium_soup.findAll('span',{'class':'a-icon-alt'})[0].text.split()[0])\n",
    "    except:\n",
    "        rating_star = 'Not Scrapable'\n",
    "    print(rating_star)\n",
    "\n",
    "    #Rating - Overall\n",
    "    try:\n",
    "        overall_rating = int(selenium_soup.findAll('span',{'id':'acrCustomerReviewText'})[0].text.split()[0].replace(',',''))\n",
    "    except:\n",
    "        overall_rating = 'Not Scrapable'\n",
    "    print(overall_rating)\n",
    "\n",
    "    #Company\n",
    "    try:\n",
    "        company = selenium_soup.findAll('a',{'id':'bylineInfo'})[0].text\n",
    "    except:\n",
    "        company = 'Not Scrapable'\n",
    "    print(country)    \n",
    "\n",
    "    #Price\n",
    "    try:\n",
    "        if country=='UAE':\n",
    "            denomination = selenium_soup.findAll('span',{'id':'priceblock_ourprice'})[0].text[:3]\n",
    "            price = float(selenium_soup.findAll('span',{'id':'priceblock_ourprice'})[0].text[3:])\n",
    "        else:\n",
    "            denomination = selenium_soup.findAll('span',{'id':'priceblock_ourprice'})[0].text[0]\n",
    "            price = float(selenium_soup.findAll('span',{'id':'priceblock_ourprice'})[0].text[1:])        \n",
    "    except:\n",
    "        try:\n",
    "            if country=='UAE':\n",
    "                try:\n",
    "                    price = float(selenium_soup.findAll('span',{'id':'priceblock_ourprice'})[0].text[3:].replace(',',''))\n",
    "                except:\n",
    "                    price = float(selenium_soup.findAll('span',{'id':'priceblock_dealprice'})[0].text[3:].replace(',',''))\n",
    "            else:\n",
    "                try:\n",
    "                    price = float(selenium_soup.findAll('span',{'id':'priceblock_ourprice'})[0].text[3:].replace(',',''))\n",
    "                except:\n",
    "                    price = float(selenium_soup.findAll('span',{'id':'priceblock_dealprice'})[0].text[3:].replace(',',''))\n",
    "        except:\n",
    "            denomination = 'Not Scrapable'\n",
    "            price = 'Not Scrapable'\n",
    "    print(denomination,price)    \n",
    "\n",
    "    #Product Highlights\n",
    "    try:\n",
    "        temp_ph = selenium_soup.findAll('ul',{'class':'a-unordered-list a-vertical a-spacing-none'})[0].findAll('li')\n",
    "        counter_ph = len(temp_ph)\n",
    "        product_highlights = []\n",
    "        for i in range(counter_ph):\n",
    "            raw = temp_ph[i].text\n",
    "            clean = raw.strip()\n",
    "            product_highlights.append(clean)\n",
    "        product_highlights = '<CPT14>'.join(product_highlights)\n",
    "    except:\n",
    "        try:\n",
    "            temp_ph = selenium_soup.findAll('div',{'id':'rich-product-description'})[0].findAll('p')\n",
    "            counter_ph = len(temp_ph)\n",
    "            product_highlights = []\n",
    "            for i in range(counter_ph):\n",
    "                raw = temp_ph[i].text\n",
    "                clean = raw.strip()\n",
    "                product_highlights.append(clean)  \n",
    "            product_highlights = '<CPT14>'.join(product_highlights)\n",
    "        except:\n",
    "            product_highlights = 'Not Available'\n",
    "    print(product_highlights)    \n",
    "    #Product Details/Dimensions:\n",
    "    #USA\n",
    "    try:\n",
    "        temp_pd = selenium_soup.findAll('div',{'class':'content'})[0].findAll('ul')[0].findAll('li')\n",
    "        counter_pd = len(temp_pd)\n",
    "        for i in range(counter_pd):\n",
    "            try:\n",
    "                if re.findall('ASIN',temp_pd[i].text)[0]:\n",
    "                    try:\n",
    "                        asin = temp_pd[i].text.split(' ')[1]\n",
    "                    except:\n",
    "                        pass\n",
    "            except IndexError:\n",
    "                pass            \n",
    "            try:\n",
    "                if re.findall('Product Dimensions|Product Dimension|Product dimensions',temp_pd[i].text)[0]:\n",
    "                    pd_temp = temp_pd[i].text.strip().split('\\n')[2].strip().split(';')\n",
    "                    try:\n",
    "                        product_length = float(pd_temp[0].split('x')[0])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_width = float(pd_temp[0].split('x')[1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_height = float(pd_temp[0].split('x')[2].split(' ')[1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        pd_unit = pd_temp[0].split('x')[2].split(' ')[2]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:    \n",
    "                        product_weight = float(pd_temp[1].split(' ')[1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        weight_unit = pd_temp[1].split(' ')[2]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            except:\n",
    "                pass                    \n",
    "            try:    \n",
    "                if re.findall('Shipping Weight|Shipping weight|shipping weight',temp_pd[i].text)[0]:\n",
    "                    sweight_temp = temp_pd[i].text.split(':')[1].strip().split(' ')\n",
    "                    shipping_weight = float(sweight_temp[0])\n",
    "                    shipping_weight_unit = sweight_temp[1] \n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                if re.findall('Amazon Best Sellers Rank|Amazon Bestsellers Rank',temp_pd[i].text)[0]:\n",
    "                    x = temp_pd[i].text.replace('\\n','').split(' ')\n",
    "                    indexes = []\n",
    "                    for j,k in enumerate(x):\n",
    "                        if re.findall('#',k):\n",
    "                            indexes.append(j)\n",
    "                    try:\n",
    "                        best_seller_cat = int(temp_pd[i].text.strip().replace('\\n','').split(' ')[3].replace(',',''))\n",
    "                        best_seller_prod = int(x[indexes[0]].split('#')[1].split('in')[0])                         \n",
    "                    except:\n",
    "                        try:\n",
    "                            best_seller_cat = x[indexes[0]].split('#')[1]\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            best_seller_prod = x[indexes[1]].split('#')[1].split('in')[0]\n",
    "                        except:\n",
    "                            pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "        print(asin)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        temp_pd = selenium_soup.findAll('div',{'class':'content'})[1].findAll('ul')[0].findAll('li')\n",
    "        counter_pd = len(temp_pd)\n",
    "        for i in range(counter_pd):\n",
    "            try:\n",
    "                if re.findall('ASIN',temp_pd[i].text)[0]:\n",
    "                    try:\n",
    "                        asin = temp_pd[i].text.split(' ')[1]\n",
    "                    except:\n",
    "                        pass\n",
    "            except IndexError:\n",
    "                pass            \n",
    "            try:\n",
    "                if re.findall('Product Dimensions|Product Dimension|Product dimensions',temp_pd[i].text)[0]:\n",
    "                    pd_temp = temp_pd[i].text.strip().split('\\n')[2].strip().split(';')\n",
    "                    try:\n",
    "                        product_length = float(pd_temp[0].split('x')[0])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_width = float(pd_temp[0].split('x')[1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_height = float(pd_temp[0].split('x')[2].split(' ')[1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        pd_unit = pd_temp[0].split('x')[2].split(' ')[2]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:    \n",
    "                        product_weight = float(pd_temp[1].split(' ')[1])\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        weight_unit = pd_temp[1].split(' ')[2]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            except:\n",
    "                pass                    \n",
    "            try:    \n",
    "                if re.findall('Shipping Weight|Shipping weight|shipping weight',temp_pd[i].text)[0]:\n",
    "                    sweight_temp = temp_pd[i].text.split(':')[1].strip().split(' ')\n",
    "                    shipping_weight = float(sweight_temp[0])\n",
    "                    shipping_weight_unit = sweight_temp[1] \n",
    "            except IndexError:\n",
    "                pass                    \n",
    "            try:\n",
    "                if re.findall('Amazon Best Sellers Rank|Amazon Bestsellers Rank',temp_pd[i].text)[0]:\n",
    "                    x = temp_pd[i].text.replace('\\n','').split(' ')\n",
    "                    indexes = []\n",
    "                    for j,k in enumerate(x):\n",
    "                        if re.findall('#',k):\n",
    "                            indexes.append(j)\n",
    "                    try:\n",
    "                        best_seller_cat = int(temp_pd[i].text.strip().replace('\\n','').split(' ')[3].replace(',',''))\n",
    "                        best_seller_prod = int(x[indexes[0]].split('#')[1].split('in')[0])                         \n",
    "                    except:\n",
    "                        try:\n",
    "                            best_seller_cat = x[indexes[0]].split('#')[1]\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            best_seller_prod = x[indexes[1]].split('#')[1].split('in')[0]\n",
    "                        except:\n",
    "                            pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "        print(asin)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #India\n",
    "    try:\n",
    "        temp_pd = selenium_soup.findAll('div',{'class':'content'})[0].findAll('ul')[0].findAll('li')\n",
    "        counter_pd = len(temp_pd)\n",
    "        for i in range(counter_pd):\n",
    "            try:\n",
    "                if re.findall('ASIN',temp_pd[i].text)[0]:\n",
    "                    asin = temp_pd[i].text.split(' ')[1]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if re.findall('Product Dimensions|Product Dimension|Product dimensions',temp_pd[i].text)[0]:\n",
    "                    pd_temp = temp_pd[i].text.strip().split('\\n')[2].strip().split(' ')\n",
    "                    try:\n",
    "                        product_length = float(pd_temp[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_width = float(pd_temp[2])\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_height = float(pd_temp[4])\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:    \n",
    "                        pd_unit = pd_temp[5]\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        product_weight = float(pd_temp[1].split(' ')[1])\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        weight_unit = pd_temp[1].split(' ')[2]\n",
    "                    except:\n",
    "                        pass\n",
    "                print(asin)\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:    \n",
    "                if re.findall('Shipping Weight|Shipping weight|shipping weight',temp_pd[i].text)[0]:\n",
    "                    sweight_temp = temp_pd[i].text.split(':')[1].strip().split(' ')\n",
    "                    shipping_weight = float(sweight_temp[0])\n",
    "                    shipping_weight_unit = sweight_temp[1] \n",
    "            except IndexError:\n",
    "                pass  \n",
    "            try:    \n",
    "                if re.findall('Item Weight|Product Weight|Item weight|Product weight|Boxed-product Weight',temp_pd[i].text)[0]:\n",
    "                    pd_weight_temp = temp_pd[i].text.replace('\\n','').strip().split('     ')[1].strip()\n",
    "                    product_weight = float(pd_weight_temp.split(' ')[0])\n",
    "                    weight_unit = pd_weight_temp.split(' ')[1]\n",
    "            except IndexError:\n",
    "                pass                \n",
    "            try:\n",
    "                if re.findall('Amazon Best Sellers Rank|Amazon Bestsellers Rank',temp_pd[i].text)[0]:\n",
    "                    x = temp_pd[i].text.strip().replace('\\n','').split(' ')\n",
    "                    indexes = []\n",
    "                    for j,k in enumerate(x):\n",
    "                        if re.findall('#',k):\n",
    "                            indexes.append(j)\n",
    "                    try:\n",
    "                        best_seller_cat = int(temp_pd[i].text.strip().replace('\\n','').split(' ')[3].replace(',',''))\n",
    "                        best_seller_prod = int(x[indexes[0]].split('#')[1].split('in')[0])                         \n",
    "                    except:                  \n",
    "                        try:\n",
    "                            best_seller_cat = x[indexes[0]].split('#')[1]\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            best_seller_prod = x[indexes[1]].split('#')[1].split('in')[0]\n",
    "                        except:\n",
    "                            pass       \n",
    "            except IndexError:\n",
    "                pass\n",
    "            print(asin)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        try:\n",
    "            asin = list(selenium_soup.findAll('div',{'class':'pdTab'})[1].findAll('tr')[0].findAll('td')[1])[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            dimensions = list(selenium_soup.findAll('div',{'class':'pdTab'})[0].findAll('tr')[0].findAll('td')[1])[0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            weight_temp = list(selenium_soup.findAll('div',{'class':'pdTab'})[1].findAll('tr')[1].findAll('td')[1])[0]\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            best_seller_cat = float(list(selenium_soup.findAll('div',{'class':'pdTab'})[1].findAll('tr')[5].findAll('td')[1])[0].split('\\n')[-1].split(' ')[0].replace(',',''))\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            best_seller_prod = int(list(list(list(list(selenium_soup.findAll('div',{'class':'pdTab'})[1].findAll('tr')[5].findAll('td')[1])[5])[1])[1])[0].replace('#',''))\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            product_length = float(dimensions.split('x')[0])\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            product_width = float(dimensions.split('x')[1])\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            product_height = float(dimensions.split('x')[2].split(' ')[1])\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            product_weight = weight_temp.split(' ')[0]\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            weight_unit = weight_temp.split(' ')[1]\n",
    "        except:\n",
    "            pass    \n",
    "        try:    \n",
    "            pd_unit = dimensions.split(' ')[-1]\n",
    "        except:\n",
    "            pass\n",
    "        print(asin)\n",
    "    except:\n",
    "        try:\n",
    "            for j in [0,1]:\n",
    "                temp_pd = selenium_soup.findAll('table',{'class':'a-keyvalue prodDetTable'})[j].findAll('tr')\n",
    "                for i in range(len(temp_pd)):\n",
    "                    if re.findall('ASIN',temp_pd[i].text):\n",
    "                        asin = temp_pd[i].text.strip().split('\\n')[3].strip()\n",
    "                    if re.findall('Item Model Number|Item model number',temp_pd[i].text):\n",
    "                        bait = temp_pd[i].text.strip().split('\\n')[3].strip()                        \n",
    "                    if re.findall('Best Sellers Rank|Amazon Best Sellers Rank|Amazon Bestsellers Rank',temp_pd[i].text):\n",
    "                        x = temp_pd[i].text.strip().replace('\\n','').split(' ')\n",
    "                        indexes = []\n",
    "                        for j,k in enumerate(x):\n",
    "                            if re.findall('#',k):\n",
    "                                indexes.append(j)\n",
    "                        best_seller_cat = int(x[indexes[0]].split('#')[1])\n",
    "                        best_seller_prod = int(x[indexes[1]].split('#')[1].split('in')[0])\n",
    "                    if re.findall('Product Dimensions|Product dimension|Product Dimension',temp_pd[i].text):\n",
    "                        dimensions = temp_pd[i].text.strip().split('\\n')[3].strip().split('x')\n",
    "                        product_length = float(dimensions[0].strip())\n",
    "                        product_width = float(dimensions[1].strip())\n",
    "                        product_height = float(dimensions[2].strip().split(' ')[0])\n",
    "                        pd_unit = dimensions[2].strip().split(' ')[1]\n",
    "                    if re.findall('Item Weight|Product Weight|Item weight|Boxed-product Weight',temp_pd[i].text):\n",
    "                        weight_temp = temp_pd[i].text.strip().split('\\n')[3].strip()\n",
    "                        product_weight = float(weight_temp.split(' ')[0])\n",
    "                        weight_unit = weight_temp.split(' ')[1]\n",
    "                    if re.findall('Shipping Weight|Shipping weight|shipping weight',temp_pd[i].text):\n",
    "                        sweight_temp = temp_pd[i].text.replace('\\n','').strip().split('                      ')[1].lstrip().split(' ')\n",
    "                        shipping_weight = float(sweight_temp[0])\n",
    "                        shipping_weight_unit = sweight_temp[1]\n",
    "                print(asin,bait)\n",
    "        except:\n",
    "            try:\n",
    "                temp_pd = selenium_soup.findAll('div',{'id':'prodDetails'})[0].findAll('tr')\n",
    "                for i in range(len(temp_pd)):\n",
    "                    if re.findall('ASIN',temp_pd[i].text):\n",
    "                        asin = temp_pd[i].text.strip().split('\\n')[3].strip()\n",
    "                    if re.findall('Best Sellers Rank|Amazon Best Sellers Rank|Amazon Bestsellers Rank',temp_pd[i].text):\n",
    "                        x = temp_pd[i].text.strip().replace('\\n','').split(' ')\n",
    "                        indexes = []\n",
    "                        for j,k in enumerate(x):\n",
    "                            if re.findall('#',k):\n",
    "                                indexes.append(j)\n",
    "                        best_seller_cat = int(x[indexes[0]].split('#')[1])\n",
    "                        best_seller_prod = int(x[indexes[1]].split('#')[1].split('in')[0])\n",
    "                    if re.findall('Product Dimensions|Product dimension|Product Dimension',temp_pd[i].text):\n",
    "                        dimensions = temp_pd[i].text.strip().split('\\n')[3].strip().split('x')\n",
    "                        product_length = float(dimensions[0].strip())\n",
    "                        product_width = float(dimensions[1].strip())\n",
    "                        product_height = float(dimensions[2].strip().split(' ')[0])\n",
    "                        pd_unit = dimensions[2].strip().split(' ')[1]\n",
    "                    if re.findall('Item Weight|Product Weight|Item weight|Boxed-product Weight',temp_pd[i].text):\n",
    "                        weight_temp = temp_pd[i].text.strip().split('\\n')[3].strip()\n",
    "                        product_weight = float(weight_temp.split(' ')[0])\n",
    "                        weight_unit = weight_temp.split(' ')[1]\n",
    "                    if re.findall('Shipping Weight|Shipping weight|shipping weight',temp_pd[i].text):\n",
    "                        sweight_temp = temp_pd[i].text.replace('\\n','').strip().split('                      ')[1].lstrip().split(' ')\n",
    "                        shipping_weight = float(sweight_temp[0])\n",
    "                        shipping_weight_unit = sweight_temp[1]\n",
    "            except:\n",
    "                try:\n",
    "                    temp_pd = selenium_soup.findAll('div',{'id':'detail_bullets_id'})[0].findAll('tr')[0].findAll('li')\n",
    "                    for i in range(len(temp_pd)):\n",
    "                        if re.findall('ASIN',temp_pd[i].text):\n",
    "                            asin = temp_pd[i].text.strip().split(':')[1].strip()\n",
    "                        if re.findall('Best Sellers Rank|Amazon Best Sellers Rank|Amazon Bestsellers Rank',temp_pd[i].text):\n",
    "                            x = temp_pd[i].text.strip().replace('\\n','').split(' ')\n",
    "                            indexes = []\n",
    "                            for j,k in enumerate(x):\n",
    "                                if re.findall('#',k):\n",
    "                                    indexes.append(j)\n",
    "                            best_seller_cat = int(x[indexes[0]].split('#')[1])\n",
    "                            best_seller_prod = int(x[indexes[1]].split('#')[1].split('in')[0])\n",
    "                        if re.findall('Product Dimensions|Product dimension|Product Dimension',temp_pd[i].text):\n",
    "                            dimensions = temp_pd[i].text.strip().split('\\n')[2].strip().split('x')\n",
    "                            product_length = float(dimensions[0].strip())\n",
    "                            product_width = float(dimensions[1].strip())\n",
    "                            product_height = float(dimensions[2].strip().split(' ')[0])\n",
    "                            pd_unit = dimensions[2].strip().split(' ')[1]\n",
    "                        if re.findall('Item Weight|Product Weight|Item weight|Boxed-product Weight',temp_pd[i].text):\n",
    "                            weight_temp = temp_pd[i].text.strip().split('\\n')[2].strip()\n",
    "                            product_weight = float(weight_temp.split(' ')[0])\n",
    "                            weight_unit = weight_temp.split(' ')[1]\n",
    "                        if re.findall('Shipping Weight|Shipping weight|shipping weight',temp_pd[i].text):\n",
    "                            sweight_temp = temp_pd[i].text.replace('\\n','').strip().split('                      ')[1].lstrip().split(' ')\n",
    "                            shipping_weight = float(sweight_temp[0])\n",
    "                            shipping_weight_unit = sweight_temp[1]\n",
    "                except:\n",
    "                    pass\n",
    "    try:\n",
    "        print(asin)\n",
    "    except NameError:\n",
    "        asin = 'Not Scrapable'\n",
    "    try:\n",
    "        print(best_seller_cat)\n",
    "    except NameError:\n",
    "        best_seller_cat = 'Not Scrapable'\n",
    "    try:\n",
    "        print(best_seller_prod)\n",
    "    except NameError:\n",
    "        best_seller_prod = 'Not Scrapable'        \n",
    "    try:\n",
    "        print(product_length)\n",
    "    except NameError:\n",
    "        product_length = 'Not Scrapable'\n",
    "    try:\n",
    "        print(product_width)\n",
    "    except NameError:\n",
    "        product_width = 'Not Scrapable'\n",
    "    try:\n",
    "        print(product_height)\n",
    "    except NameError:\n",
    "        product_height = 'Not Scrapable'\n",
    "    try:\n",
    "        print(product_weight)\n",
    "    except NameError:\n",
    "        product_weight = 'Not Scrapable'\n",
    "    try:\n",
    "        print(weight_unit)\n",
    "    except NameError:\n",
    "        weight_unit = 'Not Scrapable'\n",
    "    try:\n",
    "        print(pd_unit)\n",
    "    except NameError:\n",
    "        pd_unit = 'Not Scrapable'\n",
    "    try:\n",
    "        print(shipping_weight_unit)\n",
    "    except NameError:\n",
    "        shipping_weight_unit = 'Not Scrapable'\n",
    "    try:\n",
    "        print(shipping_weight)\n",
    "    except NameError:\n",
    "        shipping_weight = 'Not Scrapable'\n",
    "\n",
    "    print(product_length,product_width,product_height,product_weight,asin,pd_unit,\n",
    "          best_seller_cat,best_seller_prod,weight_unit,shipping_weight,shipping_weight_unit)\n",
    "\n",
    "    #Customer Review Ratings - Overall\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        temp_crr = selenium_soup.findAll('table',{'id':'histogramTable'})[1].findAll('a')\n",
    "        crr_main = {}\n",
    "        crr_temp = []\n",
    "        counter_crr = len(temp_crr)\n",
    "        for i in range(counter_crr):\n",
    "            crr_temp.append(temp_crr[i]['title'])\n",
    "        crr_temp = list(set(crr_temp))\n",
    "        for j in range(len(crr_temp)):\n",
    "            crr_temp[j] = crr_temp[j].split(' ')\n",
    "            stopwords = ['stars','represent','of','rating','reviews','have']\n",
    "            for word in list(crr_temp[j]):\n",
    "                if word in stopwords:\n",
    "                    crr_temp[j].remove(word)\n",
    "            print(crr_temp[j])\n",
    "            try:\n",
    "                if re.findall(r'%',crr_temp[j][1])[0]:\n",
    "                    crr_main.update({int(crr_temp[j][0]): int(crr_temp[j][1].replace('%',''))})\n",
    "            except:\n",
    "                crr_main.update({int(crr_temp[j][1]): int(crr_temp[j][0].replace('%',''))}) \n",
    "    except:\n",
    "        try:\n",
    "            temp_crr = selenium_soup.findAll('table',{'id':'histogramTable'})[1].findAll('span',{'class':'a-offscreen'})\n",
    "            crr_main = {}\n",
    "            counter_crr = len(temp_crr)\n",
    "            star = counter_crr\n",
    "            for i in range(counter_crr):\n",
    "                crr_main.update({star:int(temp_crr[i].text.strip().split('/n')[0].split(' ')[0].replace('%',''))})\n",
    "                star -= 1\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        crr_5 = crr_main[5]\n",
    "    except:\n",
    "        crr_5 = 0\n",
    "    try:\n",
    "        crr_4 = crr_main[4]\n",
    "    except:\n",
    "        crr_4 = 0\n",
    "    try:\n",
    "        crr_3 = crr_main[3]\n",
    "    except:\n",
    "        crr_3 = 0\n",
    "    try:\n",
    "        crr_2 = crr_main[2]\n",
    "    except:\n",
    "        crr_2 = 0\n",
    "    try:\n",
    "        crr_1 = crr_main[1]\n",
    "    except:\n",
    "        crr_1 = 0 \n",
    "\n",
    "    #Customer Review Ratings - By Feature\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"cr-summarization-attributes-list\"]/div[4]/a/span').click()\n",
    "        temp_fr = driver.find_element_by_xpath('//*[@id=\"cr-summarization-attributes-list\"]').text\n",
    "        temp_fr = temp_fr.split('\\n')\n",
    "        crr_feature_title = []\n",
    "        crr_feature_rating = []\n",
    "        for i in [0,2,4]:\n",
    "             crr_feature_title.append(temp_fr[i])\n",
    "        for j in [1,3,5]:\n",
    "            crr_feature_rating.append(temp_fr[j])\n",
    "        crr_feature = dict(zip(crr_feature_title,crr_feature_rating))\n",
    "    except:\n",
    "        try:\n",
    "            temp_fr = driver.find_element_by_xpath('//*[@id=\"cr-summarization-attributes-list\"]').text\n",
    "            temp_fr = temp_fr.split('\\n')\n",
    "            crr_feature_title = []\n",
    "            crr_feature_rating = []\n",
    "            for i in [0,2,4]:\n",
    "                 crr_feature_title.append(temp_fr[i])\n",
    "            for j in [1,3,5]:\n",
    "                crr_feature_rating.append(temp_fr[j])\n",
    "            crr_feature = dict(zip(crr_feature_title,crr_feature_rating))\n",
    "        except:\n",
    "            crr_feature = 'Not Defined'\n",
    "    try:\n",
    "        crr_feature_key = list(crr_feature.keys())\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        crr_fr_1 = crr_feature[crr_feature_key[0]]\n",
    "    except:\n",
    "        crr_fr_1 = 0\n",
    "    try:\n",
    "        crr_fr_2 = crr_feature[crr_feature_key[1]]\n",
    "    except:\n",
    "        crr_fr_2 = 0\n",
    "    try:\n",
    "        crr_fr_3 = crr_feature[crr_feature_key[2]]\n",
    "    except:\n",
    "        crr_fr_3 = 0        \n",
    "\n",
    "    #Tags:\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        temp_tags = selenium_soup.findAll('div',{'class':'cr-lighthouse-terms'})[0]\n",
    "        counter_tags = len(temp_tags)\n",
    "        print('Counter Tags:',counter_tags)\n",
    "        tags = []\n",
    "        for i in range(counter_tags):\n",
    "            tags.append(temp_tags.findAll('span')[i].text.strip())\n",
    "            print(tags[i])\n",
    "    except:\n",
    "        tags = ['None']\n",
    "    try:\n",
    "        for feature in crr_feature_key:\n",
    "            tags.append(feature)\n",
    "    except:\n",
    "        pass\n",
    "    tags = list(set(tags))\n",
    "    tags = '<CPT14>'.join(tags)\n",
    "    print(tags)\n",
    "\n",
    "\n",
    "    #Images\n",
    "    images = []\n",
    "    for i in [0,3,4,5,6,7,8,9]:\n",
    "        try:\n",
    "            images.append(selenium_soup.findAll('div',{'class':'imgTagWrapper'})[i].find('img')['src'])\n",
    "        except:\n",
    "            pass\n",
    "    import urllib.request\n",
    "    for i  in range(len(images)):\n",
    "        if asin =='Not Scrapable':\n",
    "            product_image = \"{}_{}.jpg\".format(product_title,i)\n",
    "            product_image = product_image.replace('/','')\n",
    "            urllib.request.urlretrieve(images[i],product_image)\n",
    "            upload_s3(\"{}_{}.jpg\".format(product_title,i),\n",
    "                      directory+\"/images/\" + product_image)\n",
    "            delete_images(product_image)\n",
    "        else:\n",
    "            product_image = \"{}_{}.jpg\".format(asin,i)\n",
    "            product_image = product_image.replace('/','')\n",
    "            urllib.request.urlretrieve(images[i],product_image)\n",
    "            upload_s3(\"{}_{}.jpg\".format(asin,i),\n",
    "                      directory+\"/images/\" + product_image)\n",
    "            delete_images(product_image)\n",
    "    return [product_title,rating_star,overall_rating,company,price,\n",
    "            product_highlights,product_length,product_width,product_height,\n",
    "            product_weight,asin,pd_unit,best_seller_cat,best_seller_prod,\n",
    "            weight_unit,shipping_weight,shipping_weight_unit,crr_5,crr_4,\n",
    "            crr_3,crr_2,crr_1,crr_fr_1,crr_fr_2,crr_fr_3,tags,directory]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:03.188325Z",
     "start_time": "2020-03-20T14:42:03.180094Z"
    }
   },
   "outputs": [],
   "source": [
    "def database(product_data,**kwargs):   \n",
    "    try:\n",
    "        try:\n",
    "            link = kwargs['link']\n",
    "        except KeyError:\n",
    "            print('Error in Link')\n",
    "        try:\n",
    "            country = kwargs['country']\n",
    "        except KeyError:\n",
    "            print(\"Enter Country Name\")\n",
    "        try:\n",
    "            cat1 = kwargs['cat1']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            cat2 = kwargs['cat2']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            cat3 = kwargs['cat3']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            cat4 = kwargs['cat4']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            product = kwargs['product']\n",
    "        except KeyError:\n",
    "            print(\"Enter Product Name\")\n",
    "        metadata = [link,country,cat1,cat2,cat3,cat4,product]\n",
    "    except NameError:\n",
    "        try:\n",
    "            cat4 = None\n",
    "            metadata = [link,country,cat1,cat2,cat3,cat4,product]        \n",
    "        except NameError:\n",
    "            try:\n",
    "                cat4 = None\n",
    "                cat3 = None\n",
    "                metadata = [link,country,cat1,cat2,cat3,cat4,product]\n",
    "            except NameError:\n",
    "                cat4 = None\n",
    "                cat3 = None\n",
    "                cat2 = None\n",
    "                metadata = [link,country,cat1,cat2,cat3,cat4,product]    \n",
    "    conn = sqlite3.connect('{}.db'.format(product))\n",
    "    headers = ['link','country','cat1','cat2','cat3','cat4','product','product_title',\n",
    "               'rating_star','overall_rating','company','price',\n",
    "               'product_highlights','product_length','product_width','product_height',\n",
    "               'product_weight','asin','pd_unit','best_seller_cat','best_seller_prod',\n",
    "               'weight_unit','shipping_weight','shipping_weight_unit','crr_5','crr_4',\n",
    "               'crr_3','crr_2','crr_1','crr_fr_1','crr_fr_2','crr_fr_3','tags','images_link']\n",
    "    product_data.append(metadata)\n",
    "    product_data = product_data[-1] + product_data[:len(product_data)-1]\n",
    "    temp = pd.DataFrame(data= [product_data],columns=headers)\n",
    "    temp.to_sql('Product',conn,if_exists='append')\n",
    "    upload_s3(product+'.db',directory+'/'+product+'.db')\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:42:03.195340Z",
     "start_time": "2020-03-20T14:42:03.189399Z"
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint(link_list,directory,product):\n",
    "    BUCKET_NAME = 'amazon-data-ecfullfill' \n",
    "    key_id = 'AKIAWR6YW7N5ZKW35OJI'\n",
    "    access_key = 'h/xrcI9A2SRU0ds+zts4EClKAqbzU+/iXdiDcgzm'\n",
    "    KEY = '{}/{}.db'.format(directory,product)\n",
    "    s3 = boto3.resource('s3',aws_access_key_id=key_id,\n",
    "                          aws_secret_access_key=access_key)\n",
    "    try:\n",
    "        s3.Bucket(BUCKET_NAME).download_file(KEY, 'test.db')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"The object does not exist.\")\n",
    "        else:\n",
    "            raise\n",
    "    conn = sqlite3.connect('test.db')    \n",
    "    try:\n",
    "        df = pd.read_sql('''SELECT * FROM Product''', conn)\n",
    "        product_link = df['link'].unique()\n",
    "        new_list = []\n",
    "        for i in link_list:\n",
    "            if i in product_link:\n",
    "                pass\n",
    "            else:\n",
    "                new_list.append(i)\n",
    "    except:\n",
    "        new_list = link_list    \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:40:55.283748Z",
     "start_time": "2020-03-08T11:40:55.082585Z"
    }
   },
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T15:16:24.936374Z",
     "start_time": "2020-03-21T15:16:24.933936Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health_and_beauty/skin_care/face/f_treatments\n",
      "400\n",
      "1\n",
      "Not Scrapable\n",
      "2\n",
      "Not Scrapable\n",
      "3\n",
      "Not Scrapable\n",
      "4\n",
      "Not Scrapable\n",
      "5\n",
      "Not Scrapable\n",
      "6\n",
      "Not Scrapable\n",
      "7\n",
      "Not Scrapable\n",
      "8\n",
      "Not Scrapable\n",
      "9\n",
      "Not Scrapable\n",
      "10\n",
      "Not Scrapable\n",
      "11\n",
      "Not Scrapable\n",
      "12\n",
      "Not Scrapable\n",
      "13\n",
      "Not Scrapable\n",
      "14\n",
      "Not Scrapable\n",
      "15\n",
      "Not Scrapable\n",
      "16\n",
      "Not Scrapable\n",
      "17\n",
      "Not Scrapable\n",
      "18\n",
      "Not Scrapable\n",
      "19\n",
      "Not Scrapable\n",
      "20\n",
      "Not Scrapable\n",
      "21\n",
      "Not Scrapable\n",
      "22\n",
      "Not Scrapable\n",
      "23\n",
      "Not Scrapable\n",
      "24\n",
      "Not Scrapable\n",
      "25\n",
      "Not Scrapable\n",
      "26\n",
      "Not Scrapable\n",
      "27\n",
      "Not Scrapable\n",
      "28\n",
      "Not Scrapable\n",
      "29\n",
      "Not Scrapable\n",
      "30\n",
      "Not Scrapable\n",
      "31\n",
      "Not Scrapable\n",
      "32\n",
      "Not Scrapable\n",
      "33\n",
      "Not Scrapable\n",
      "34\n",
      "Not Scrapable\n",
      "35\n",
      "Not Scrapable\n",
      "36\n",
      "Not Scrapable\n",
      "37\n",
      "Not Scrapable\n",
      "38\n",
      "Not Scrapable\n",
      "39\n",
      "Not Scrapable\n",
      "40\n",
      "Not Scrapable\n",
      "41\n",
      "Not Scrapable\n",
      "42\n",
      "Not Scrapable\n",
      "43\n",
      "Not Scrapable\n",
      "44\n",
      "Not Scrapable\n",
      "45\n",
      "Not Scrapable\n",
      "46\n",
      "Not Scrapable\n",
      "47\n",
      "Not Scrapable\n",
      "48\n",
      "Not Scrapable\n",
      "49\n",
      "Not Scrapable\n",
      "50\n",
      "Not Scrapable\n",
      "51\n",
      "Not Scrapable\n",
      "52\n",
      "Not Scrapable\n",
      "53\n",
      "Not Scrapable\n",
      "54\n",
      "Not Scrapable\n",
      "55\n",
      "Not Scrapable\n",
      "56\n",
      "Not Scrapable\n",
      "57\n",
      "Not Scrapable\n",
      "58\n",
      "Not Scrapable\n",
      "59\n",
      "Not Scrapable\n",
      "60\n",
      "Not Scrapable\n",
      "61\n",
      "Not Scrapable\n",
      "62\n",
      "Not Scrapable\n",
      "63\n",
      "Not Scrapable\n",
      "64\n",
      "Not Scrapable\n",
      "65\n",
      "Not Scrapable\n",
      "66\n",
      "Not Scrapable\n",
      "67\n",
      "Not Scrapable\n",
      "68\n",
      "Not Scrapable\n",
      "69\n",
      "Not Scrapable\n",
      "70\n",
      "Not Scrapable\n",
      "71\n",
      "Not Scrapable\n",
      "72\n",
      "Not Scrapable\n",
      "73\n",
      "Not Scrapable\n",
      "74\n",
      "Not Scrapable\n",
      "75\n",
      "Not Scrapable\n",
      "76\n",
      "Not Scrapable\n",
      "77\n",
      "Not Scrapable\n",
      "78\n",
      "Not Scrapable\n",
      "79\n",
      "Not Scrapable\n",
      "80\n",
      "Not Scrapable\n",
      "81\n",
      "Not Scrapable\n",
      "82\n",
      "Not Scrapable\n",
      "83\n",
      "Not Scrapable\n",
      "84\n",
      "Not Scrapable\n",
      "85\n",
      "Not Scrapable\n",
      "86\n",
      "Not Scrapable\n",
      "87\n",
      "Not Scrapable\n",
      "88\n",
      "Not Scrapable\n",
      "89\n",
      "Not Scrapable\n",
      "90\n",
      "Not Scrapable\n",
      "91\n",
      "Not Scrapable\n",
      "92\n",
      "Not Scrapable\n",
      "93\n",
      "Not Scrapable\n",
      "94\n",
      "Not Scrapable\n",
      "95\n",
      "Not Scrapable\n",
      "96\n",
      "Not Scrapable\n",
      "97\n",
      "Not Scrapable\n",
      "98\n",
      "Not Scrapable\n",
      "99\n",
      "Not Scrapable\n",
      "100\n",
      "Not Scrapable\n",
      "101\n",
      "Not Scrapable\n",
      "102\n",
      "Not Scrapable\n",
      "103\n",
      "Not Scrapable\n",
      "104\n",
      "Not Scrapable\n",
      "105\n",
      "Not Scrapable\n",
      "106\n",
      "Not Scrapable\n",
      "107\n",
      "Not Scrapable\n",
      "108\n",
      "Not Scrapable\n",
      "109\n",
      "Not Scrapable\n",
      "110\n",
      "Not Scrapable\n",
      "111\n",
      "Not Scrapable\n",
      "112\n",
      "Not Scrapable\n",
      "113\n",
      "Not Scrapable\n",
      "114\n",
      "Not Scrapable\n",
      "115\n",
      "Not Scrapable\n",
      "116\n",
      "Not Scrapable\n",
      "117\n",
      "Not Scrapable\n",
      "118\n",
      "Not Scrapable\n",
      "119\n",
      "Not Scrapable\n",
      "120\n",
      "Not Scrapable\n",
      "121\n",
      "Not Scrapable\n",
      "122\n",
      "Not Scrapable\n",
      "123\n",
      "Not Scrapable\n",
      "124\n",
      "Not Scrapable\n",
      "125\n",
      "Not Scrapable\n",
      "126\n",
      "48\n",
      "96\n",
      "127\n",
      "Not Scrapable\n",
      "128\n",
      "Not Scrapable\n",
      "129\n",
      "Not Scrapable\n",
      "130\n",
      "Not Scrapable\n",
      "131\n",
      "Not Scrapable\n",
      "132\n",
      "48\n",
      "96\n",
      "133\n",
      "Not Scrapable\n",
      "134\n",
      "Not Scrapable\n",
      "135\n",
      "Not Scrapable\n",
      "136\n",
      "Not Scrapable\n",
      "137\n",
      "Not Scrapable\n",
      "138\n",
      "Not Scrapable\n",
      "139\n",
      "Not Scrapable\n",
      "140\n",
      "Not Scrapable\n",
      "141\n",
      "Not Scrapable\n",
      "142\n",
      "Not Scrapable\n",
      "143\n",
      "Not Scrapable\n",
      "144\n",
      "Not Scrapable\n",
      "145\n",
      "Not Scrapable\n",
      "146\n",
      "Not Scrapable\n",
      "147\n",
      "Not Scrapable\n",
      "148\n",
      "Not Scrapable\n",
      "149\n",
      "Not Scrapable\n",
      "150\n",
      "Not Scrapable\n",
      "151\n",
      "Not Scrapable\n",
      "152\n",
      "Not Scrapable\n",
      "153\n",
      "Not Scrapable\n",
      "154\n",
      "Not Scrapable\n",
      "155\n",
      "Not Scrapable\n",
      "156\n",
      "Not Scrapable\n",
      "157\n",
      "Not Scrapable\n",
      "158\n",
      "Not Scrapable\n",
      "159\n",
      "Not Scrapable\n",
      "160\n",
      "Not Scrapable\n",
      "161\n",
      "Not Scrapable\n",
      "162\n",
      "Not Scrapable\n",
      "163\n",
      "Not Scrapable\n",
      "164\n",
      "Not Scrapable\n",
      "165\n",
      "Not Scrapable\n",
      "166\n",
      "Not Scrapable\n",
      "167\n",
      "Not Scrapable\n",
      "168\n",
      "48\n",
      "96\n",
      "169\n",
      "Not Scrapable\n",
      "170\n",
      "Not Scrapable\n",
      "171\n",
      "Not Scrapable\n",
      "172\n",
      "Not Scrapable\n",
      "173\n",
      "Not Scrapable\n",
      "174\n",
      "Not Scrapable\n",
      "175\n",
      "Not Scrapable\n",
      "176\n",
      "Not Scrapable\n",
      "177\n",
      "Not Scrapable\n",
      "178\n",
      "Not Scrapable\n",
      "179\n",
      "Not Scrapable\n",
      "180\n",
      "Not Scrapable\n",
      "181\n",
      "Not Scrapable\n",
      "182\n",
      "Not Scrapable\n",
      "183\n",
      "Not Scrapable\n",
      "184\n",
      "Not Scrapable\n",
      "185\n",
      "Not Scrapable\n",
      "186\n",
      "Not Scrapable\n",
      "187\n",
      "Not Scrapable\n",
      "188\n",
      "Not Scrapable\n",
      "189\n",
      "Not Scrapable\n",
      "190\n",
      "48\n",
      "96\n",
      "191\n",
      "Not Scrapable\n",
      "192\n",
      "Not Scrapable\n",
      "193\n",
      "Not Scrapable\n",
      "194\n",
      "Not Scrapable\n",
      "195\n",
      "Not Scrapable\n",
      "196\n",
      "Not Scrapable\n",
      "197\n",
      "Not Scrapable\n",
      "198\n",
      "Not Scrapable\n",
      "199\n",
      "Not Scrapable\n",
      "200\n",
      "Not Scrapable\n",
      "201\n",
      "Not Scrapable\n",
      "202\n",
      "Not Scrapable\n",
      "203\n",
      "Not Scrapable\n",
      "204\n",
      "Not Scrapable\n",
      "205\n",
      "Not Scrapable\n",
      "206\n",
      "Not Scrapable\n",
      "207\n",
      "Not Scrapable\n",
      "208\n",
      "Not Scrapable\n",
      "209\n",
      "Not Scrapable\n",
      "210\n",
      "Not Scrapable\n",
      "211\n",
      "Not Scrapable\n",
      "212\n",
      "48\n",
      "96\n",
      "213\n",
      "Not Scrapable\n",
      "214\n",
      "Not Scrapable\n",
      "215\n",
      "Not Scrapable\n",
      "216\n",
      "Not Scrapable\n",
      "217\n",
      "Not Scrapable\n",
      "218\n",
      "Not Scrapable\n",
      "219\n",
      "Not Scrapable\n",
      "220\n",
      "Not Scrapable\n",
      "221\n",
      "Not Scrapable\n",
      "222\n",
      "Not Scrapable\n",
      "223\n",
      "Not Scrapable\n",
      "224\n",
      "Not Scrapable\n",
      "225\n",
      "Not Scrapable\n",
      "226\n",
      "48\n",
      "96\n",
      "227\n",
      "Not Scrapable\n",
      "228\n",
      "Not Scrapable\n",
      "229\n",
      "Not Scrapable\n",
      "230\n",
      "Not Scrapable\n",
      "231\n",
      "Not Scrapable\n",
      "232\n",
      "Not Scrapable\n",
      "233\n",
      "Not Scrapable\n",
      "234\n",
      "Not Scrapable\n",
      "235\n",
      "Not Scrapable\n",
      "236\n",
      "Not Scrapable\n",
      "237\n",
      "Not Scrapable\n",
      "238\n",
      "Not Scrapable\n",
      "239\n",
      "Not Scrapable\n",
      "240\n",
      "Not Scrapable\n",
      "241\n",
      "48\n",
      "96\n",
      "242\n",
      "Not Scrapable\n",
      "243\n",
      "Not Scrapable\n",
      "244\n",
      "Not Scrapable\n",
      "245\n",
      "Not Scrapable\n",
      "246\n",
      "Not Scrapable\n",
      "247\n",
      "Not Scrapable\n",
      "248\n",
      "Not Scrapable\n",
      "249\n",
      "Not Scrapable\n",
      "250\n",
      "Not Scrapable\n",
      "268\n",
      "Not Scrapable\n",
      "269\n",
      "Not Scrapable\n",
      "270\n",
      "Not Scrapable\n",
      "271\n",
      "Not Scrapable\n",
      "272\n",
      "48\n",
      "96\n",
      "273\n",
      "Not Scrapable\n",
      "274\n",
      "Not Scrapable\n",
      "275\n",
      "Not Scrapable\n",
      "276\n",
      "Not Scrapable\n",
      "277\n",
      "Not Scrapable\n",
      "278\n",
      "Not Scrapable\n",
      "279\n",
      "Not Scrapable\n",
      "280\n",
      "Not Scrapable\n",
      "281\n",
      "Not Scrapable\n",
      "282\n",
      "Not Scrapable\n",
      "283\n",
      "Not Scrapable\n",
      "284\n",
      "Not Scrapable\n",
      "285\n",
      "Not Scrapable\n",
      "286\n",
      "Not Scrapable\n",
      "287\n",
      "Not Scrapable\n",
      "288\n",
      "Not Scrapable\n",
      "289\n",
      "Not Scrapable\n",
      "290\n",
      "Not Scrapable\n",
      "291\n",
      "Not Scrapable\n",
      "292\n",
      "Not Scrapable\n",
      "293\n",
      "Not Scrapable\n",
      "294\n",
      "Not Scrapable\n",
      "295\n",
      "Not Scrapable\n",
      "296\n",
      "Not Scrapable\n",
      "297\n",
      "48\n",
      "96\n",
      "298\n",
      "Not Scrapable\n",
      "299\n",
      "Not Scrapable\n",
      "300\n",
      "Not Scrapable\n",
      "301\n",
      "48\n",
      "96\n",
      "302\n",
      "Not Scrapable\n",
      "303\n",
      "Not Scrapable\n",
      "304\n",
      "48\n",
      "96\n",
      "305\n",
      "Not Scrapable\n",
      "306\n",
      "Not Scrapable\n",
      "307\n",
      "Not Scrapable\n",
      "308\n",
      "Not Scrapable\n",
      "309\n",
      "Not Scrapable\n",
      "310\n",
      "48\n",
      "96\n",
      "311\n",
      "Not Scrapable\n",
      "312\n",
      "Not Scrapable\n",
      "313\n",
      "Not Scrapable\n",
      "314\n",
      "Not Scrapable\n",
      "315\n",
      "Not Scrapable\n",
      "316\n",
      "Not Scrapable\n",
      "317\n",
      "Not Scrapable\n",
      "318\n",
      "Not Scrapable\n",
      "319\n",
      "Not Scrapable\n",
      "320\n",
      "Not Scrapable\n",
      "321\n",
      "Not Scrapable\n",
      "322\n",
      "Not Scrapable\n",
      "323\n",
      "Not Scrapable\n",
      "324\n",
      "Not Scrapable\n",
      "325\n",
      "Not Scrapable\n",
      "326\n",
      "Not Scrapable\n",
      "327\n",
      "Not Scrapable\n",
      "328\n",
      "Not Scrapable\n",
      "329\n",
      "Not Scrapable\n",
      "330\n",
      "Not Scrapable\n",
      "331\n",
      "Not Scrapable\n",
      "332\n",
      "Not Scrapable\n",
      "333\n",
      "Not Scrapable\n",
      "334\n",
      "Not Scrapable\n",
      "335\n",
      "Not Scrapable\n",
      "336\n",
      "Not Scrapable\n",
      "337\n",
      "Not Scrapable\n",
      "338\n",
      "Not Scrapable\n",
      "339\n",
      "Not Scrapable\n",
      "340\n",
      "Not Scrapable\n",
      "341\n",
      "Not Scrapable\n",
      "342\n",
      "Not Scrapable\n",
      "343\n",
      "Not Scrapable\n",
      "344\n",
      "Not Scrapable\n",
      "345\n",
      "Not Scrapable\n",
      "346\n",
      "Not Scrapable\n",
      "347\n",
      "Not Scrapable\n",
      "348\n",
      "Not Scrapable\n",
      "349\n",
      "Not Scrapable\n",
      "350\n",
      "Not Scrapable\n",
      "351\n",
      "48\n",
      "96\n",
      "352\n",
      "Not Scrapable\n",
      "353\n",
      "Not Scrapable\n",
      "354\n",
      "Not Scrapable\n",
      "355\n",
      "Not Scrapable\n",
      "356\n",
      "Not Scrapable\n",
      "357\n",
      "Not Scrapable\n",
      "358\n",
      "Not Scrapable\n",
      "359\n",
      "Not Scrapable\n",
      "360\n",
      "Not Scrapable\n",
      "361\n",
      "Not Scrapable\n",
      "362\n",
      "Not Scrapable\n",
      "363\n",
      "Not Scrapable\n",
      "364\n",
      "Not Scrapable\n",
      "365\n",
      "Not Scrapable\n",
      "366\n",
      "Not Scrapable\n",
      "367\n",
      "Not Scrapable\n",
      "368\n",
      "Not Scrapable\n",
      "369\n",
      "Not Scrapable\n",
      "370\n",
      "Not Scrapable\n",
      "371\n",
      "Not Scrapable\n",
      "372\n",
      "Not Scrapable\n",
      "373\n",
      "Not Scrapable\n",
      "374\n",
      "Not Scrapable\n",
      "375\n",
      "Not Scrapable\n",
      "376\n",
      "Not Scrapable\n",
      "377\n",
      "Not Scrapable\n",
      "378\n",
      "Not Scrapable\n",
      "379\n",
      "Not Scrapable\n",
      "380\n",
      "Not Scrapable\n",
      "381\n",
      "Not Scrapable\n",
      "382\n",
      "Not Scrapable\n",
      "383\n",
      "Not Scrapable\n",
      "384\n",
      "48\n",
      "96\n",
      "385\n",
      "Not Scrapable\n",
      "386\n",
      "Not Scrapable\n",
      "387\n",
      "Not Scrapable\n",
      "388\n",
      "Not Scrapable\n",
      "389\n",
      "Not Scrapable\n",
      "390\n",
      "Not Scrapable\n",
      "391\n",
      "Not Scrapable\n",
      "392\n",
      "Not Scrapable\n",
      "393\n",
      "Not Scrapable\n",
      "394\n",
      "Not Scrapable\n",
      "395\n",
      "Not Scrapable\n",
      "396\n",
      "Not Scrapable\n",
      "397\n",
      "Not Scrapable\n",
      "398\n",
      "Not Scrapable\n",
      "399\n",
      "Not Scrapable\n",
      "400\n",
      "Not Scrapable\n"
     ]
    }
   ],
   "source": [
    "#Initializing the product per Jupyter Notebook\n",
    "country = 'UK'\n",
    "cat1 = 'health_and_beauty'\n",
    "cat2='skin_care'\n",
    "cat3='face'\n",
    "# cat4 = 'None'\n",
    "product='f_treatments'\n",
    "\n",
    "links,directory = products_links(country=country,category=cat1,cat2=cat2,cat3=cat3,product=product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T08:57:05.585300Z",
     "start_time": "2020-03-15T08:57:05.575645Z"
    }
   },
   "outputs": [],
   "source": [
    "test_1 = {'links':links,'directory':directory}\n",
    "with open('uk_skin_care_face_treatments.pkl', 'wb') as f:\n",
    "    pickle.dump(test_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T15:16:28.433028Z",
     "start_time": "2020-03-21T15:16:28.407403Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('uk_skin_care_face_treatments.pkl', 'rb') as f:\n",
    "    file = pickle.load(f)\n",
    "links = file['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T15:16:33.620204Z",
     "start_time": "2020-03-21T15:16:33.617875Z"
    }
   },
   "outputs": [],
   "source": [
    "directory = 'Amazon_UK/health_and_beauty/skin_care/face/f_treatments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-21T15:16:41.159Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#replace links with new_links if interruption\n",
    "for link in new_links:\n",
    "    data = product_info(link=link,directory=directory,country=country)\n",
    "    conn = sqlite3.connect('{}.db'.format(product))\n",
    "    database(product_data=data,link=link,country=country,\n",
    "             cat1=cat1,cat2=cat2,product=product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T15:16:38.846085Z",
     "start_time": "2020-03-21T15:16:37.747099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Run if there is an interruption\n",
    "new_links = checkpoint(links,directory,product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:59:41.355050Z",
     "start_time": "2020-03-14T11:59:41.351273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17144"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:59:42.309462Z",
     "start_time": "2020-03-14T11:59:42.306141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the datasets in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T05:17:23.117029Z",
     "start_time": "2020-03-15T05:17:23.004242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object does not exist.\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM Product': no such table: Product",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/home/ubuntu/miniconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: Product",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e628d8adb77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'treatments.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlink_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM Product\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/miniconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/miniconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/miniconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM Product': no such table: Product"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = 'amazon-data-ecfullfill' # replace with your bucket name\n",
    "key_id = 'AKIAWR6YW7N5ZKW35OJI'\n",
    "access_key = 'h/xrcI9A2SRU0ds+zts4EClKAqbzU+/iXdiDcgzm'\n",
    "KEY = 'Amazon_AU/health_and_beauty/skin_care/body/treatments/treatments.db' # replace with your object key\n",
    "\n",
    "s3 = boto3.resource('s3',aws_access_key_id=key_id,\n",
    "                      aws_secret_access_key=access_key)\n",
    "\n",
    "try:\n",
    "    s3.Bucket(BUCKET_NAME).download_file(KEY, 'test.db')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"The object does not exist.\")\n",
    "    else:\n",
    "        raise\n",
    "conn = sqlite3.connect('treatments.db')\n",
    "link_db = pd.read_sql(\"SELECT * FROM Product\",conn)['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def upload_s3(filename,key):\n",
    "#     key_id = 'AKIAWR6YW7N5ZKW35OJI'\n",
    "#     access_key = 'h/xrcI9A2SRU0ds+zts4EClKAqbzU+/iXdiDcgzm'\n",
    "#     bucket_name = 'amazon-data-ecfullfill'\n",
    "#     s3 = boto3.client('s3',aws_access_key_id=key_id,\n",
    "#                       aws_secret_access_key=access_key)\n",
    "# #     s3.put_object(Bucket=bucket_name, Key='Amazon/health_and_beauty/hair_product/shampoo')\n",
    "#     s3.upload_file(filename,bucket_name,key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
